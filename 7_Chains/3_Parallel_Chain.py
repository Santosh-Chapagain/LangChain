from langchain_google_genai import ChatGoogleGenerativeAI
from dotenv import load_dotenv
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_huggingface import ChatHuggingFace , HuggingFaceEndpoint
import os
from langchain_core.runnables import RunnableParallel

load_dotenv()


model1 = ChatGoogleGenerativeAI(model='gemini-2.5-flash')

llm = HuggingFaceEndpoint(
    repo_id="mistralai/Mistral-7B-Instruct-v0.2",
    huggingfacehub_api_token=os.getenv("HUGGINGFACEHUB_ACCESS_TOKEN"),
)

model2 = ChatHuggingFace(llm=llm)


prompt1 = PromptTemplate(
    template='Generate short and simple note from the following text \n {text}',
    input_variables=['text']

)

prompt2 = PromptTemplate(
    template= 'Generate 5 short question answer from the following text \n {text}' ,
    input_variables= ['text']

)

prompt3 = PromptTemplate(
    template='Merge the provided notes and quizes into a single document \n notes => {notes} and quizes => {quiz} ' ,
    input_variables=['notes' , 'quiz']
)

parser = StrOutputParser()

parallel_chain = RunnableParallel({
    'notes': prompt1 | model2 | parser ,
    'quiz': prompt2 | model1 | parser

})

merge_chain = prompt3 | model1 | parser 
chain = parallel_chain | merge_chain 

text = """
Support Vector Machine (SVM)
1. What is SVM?

A Support Vector Machine (SVM) is a supervised machine learning algorithm used for:

Classification

Regression

Outlier detection

SVM finds the best decision boundary (a line in 2D, plane in 3D, hyperplane in higher dimensions) that separates data points of different classes with the maximum possible margin.

The goal is not just to separate classes, but to separate them as confidently as possible.

2. Geometric Intuition

Suppose you have two classes:

Class +1 (positive)

Class âˆ’1 (negative)

There are infinitely many lines that can separate them.

SVM chooses the one that:

Maximizes the distance from the closest points of both classes.

These closest points are called Support Vectors â€” they support or define the position of the boundary.

The margin is the distance between:

The closest positive point

The closest negative point

A larger margin means:

Better generalization

More robustness to noise

3. Mathematical Representation

Let the training data be:

{
(
ğ‘¥
1
,
ğ‘¦
1
)
,
(
ğ‘¥
2
,
ğ‘¦
2
)
,
.
.
.
,
(
ğ‘¥
ğ‘›
,
ğ‘¦
ğ‘›
)
}
{(x
1
	â€‹

,y
1
	â€‹

),(x
2
	â€‹

,y
2
	â€‹

),...,(x
n
	â€‹

,y
n
	â€‹

)}

where

ğ‘¥
ğ‘–
âˆˆ
ğ‘…
ğ‘‘
,
ğ‘¦
ğ‘–
âˆˆ
{
+
1
,
âˆ’
1
}
x
i
	â€‹

âˆˆR
d
,y
i
	â€‹

âˆˆ{+1,âˆ’1}

We want to find a hyperplane:

ğ‘¤
â‹…
ğ‘¥
+
ğ‘
=
0
wâ‹…x+b=0

where

ğ‘¤
w = weight vector

ğ‘
b = bias

Classification rule:

ğ‘¦
=
sign
(
ğ‘¤
â‹…
ğ‘¥
+
ğ‘
)
y=sign(wâ‹…x+b)
4. Margin

The distance of a point 
ğ‘¥
x from the hyperplane is:

âˆ£
ğ‘¤
â‹…
ğ‘¥
+
ğ‘
âˆ£
âˆ¥
ğ‘¤
âˆ¥
âˆ¥wâˆ¥
âˆ£wâ‹…x+bâˆ£
	â€‹


The margin (distance between the two class boundaries) is:

2
âˆ¥
ğ‘¤
âˆ¥
âˆ¥wâˆ¥
2
	â€‹


So maximizing margin is equivalent to minimizing:

âˆ¥
ğ‘¤
âˆ¥
âˆ¥wâˆ¥
5. Hard-Margin SVM (Perfectly Separable Data)

We impose constraints:

ğ‘¦
ğ‘–
(
ğ‘¤
â‹…
ğ‘¥
ğ‘–
+
ğ‘
)
â‰¥
1
y
i
	â€‹

(wâ‹…x
i
	â€‹

+b)â‰¥1

Optimization problem:

min
â¡
ğ‘¤
,
ğ‘
1
2
âˆ¥
ğ‘¤
âˆ¥
2
w,b
min
	â€‹

2
1
	â€‹

âˆ¥wâˆ¥
2

subject to

ğ‘¦
ğ‘–
(
ğ‘¤
â‹…
ğ‘¥
ğ‘–
+
ğ‘
)
â‰¥
1
y
i
	â€‹

(wâ‹…x
i
	â€‹

+b)â‰¥1

This finds the maximum-margin hyperplane.

6. Soft-Margin SVM (Real Data)

Real data has noise, so we allow errors using slack variables 
ğœ‰
ğ‘–
Î¾
i
	â€‹

.

Constraints:

ğ‘¦
ğ‘–
(
ğ‘¤
â‹…
ğ‘¥
ğ‘–
+
ğ‘
)
â‰¥
1
âˆ’
ğœ‰
ğ‘–
y
i
	â€‹

(wâ‹…x
i
	â€‹

+b)â‰¥1âˆ’Î¾
i
	â€‹


Objective:

min
â¡
ğ‘¤
,
ğ‘
,
ğœ‰
(
1
2
âˆ¥
ğ‘¤
âˆ¥
2
+
ğ¶
âˆ‘
ğœ‰
ğ‘–
)
w,b,Î¾
min
	â€‹

(
2
1
	â€‹

âˆ¥wâˆ¥
2
+Câˆ‘Î¾
i
	â€‹

)

Here:

ğ¶
C controls trade-off between margin size and misclassification

Large 
ğ¶
C â†’ less error allowed

Small 
ğ¶
C â†’ more tolerance

7. Dual Formulation

Using Lagrange multipliers 
ğ›¼
ğ‘–
Î±
i
	â€‹

, we convert to:

max
â¡
ğ›¼
âˆ‘
ğ›¼
ğ‘–
âˆ’
1
2
âˆ‘
ğ›¼
ğ‘–
ğ›¼
ğ‘—
ğ‘¦
ğ‘–
ğ‘¦
ğ‘—
(
ğ‘¥
ğ‘–
â‹…
ğ‘¥
ğ‘—
)
Î±
max
	â€‹

âˆ‘Î±
i
	â€‹

âˆ’
2
1
	â€‹

âˆ‘Î±
i
	â€‹

Î±
j
	â€‹

y
i
	â€‹

y
j
	â€‹

(x
i
	â€‹

â‹…x
j
	â€‹

)

subject to:

âˆ‘
ğ›¼
ğ‘–
ğ‘¦
ğ‘–
=
0
,
ğ›¼
ğ‘–
â‰¥
0
âˆ‘Î±
i
	â€‹

y
i
	â€‹

=0,Î±
i
	â€‹

â‰¥0

Only data points with 
ğ›¼
ğ‘–
>
0
Î±
i
	â€‹

>0 become support vectors.

8. Kernel Trick (Non-linear SVM)

When data is not linearly separable, SVM maps it to a higher-dimensional space:

ğ‘¥
â†’
ğœ™
(
ğ‘¥
)
xâ†’Ï•(x)

Instead of computing 
ğœ™
(
ğ‘¥
)
Ï•(x) explicitly, we use a kernel:

ğ¾
(
ğ‘¥
ğ‘–
,
ğ‘¥
ğ‘—
)
=
ğœ™
(
ğ‘¥
ğ‘–
)
â‹…
ğœ™
(
ğ‘¥
ğ‘—
)
K(x
i
	â€‹

,x
j
	â€‹

)=Ï•(x
i
	â€‹

)â‹…Ï•(x
j
	â€‹

)

Common kernels:

Kernel	Formula
Linear	
ğ‘¥
ğ‘–
â‹…
ğ‘¥
ğ‘—
x
i
	â€‹

â‹…x
j
	â€‹


Polynomial	
(
ğ‘¥
ğ‘–
â‹…
ğ‘¥
ğ‘—
+
1
)
ğ‘‘
(x
i
	â€‹

â‹…x
j
	â€‹

+1)
d

RBF (Gaussian)	
ğ‘’
âˆ’
ğ›¾
âˆ¥
ğ‘¥
ğ‘–
âˆ’
ğ‘¥
ğ‘—
âˆ¥
2
e
âˆ’Î³âˆ¥x
i
	â€‹

âˆ’x
j
	â€‹

âˆ¥
2

Sigmoid	
tanh
â¡
(
ğ‘¥
ğ‘–
â‹…
ğ‘¥
ğ‘—
)
tanh(x
i
	â€‹

â‹…x
j
	â€‹

)

This allows SVM to create non-linear decision boundaries.

9. Why SVM is Powerful

SVM:

Works well in high-dimensional spaces

Avoids overfitting via maximum margin

Uses only support vectors â†’ efficient memory use

Handles non-linear data using kernels

10. Summary

Support Vector Machine finds the hyperplane that:

Separates classes

Maximizes margin

Uses only critical data points (support vectors)

Can be extended to non-linear data via kernels

It is one of the most mathematically elegant and powerful classifiers in machine learning.

"""

result = chain.invoke({'text': text})
print(result)